{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Closed-Loop Evaluation\n",
    "In this notebook you are going to evaluate a CNN-based policy to control the SDV with a protocol named *closed-loop* evaluation.\n",
    "\n",
    "**Note: this notebook assumes you've already run the [training notebook](./train.ipynb) and stored your model successfully.**\n",
    "\n",
    "## What is closed-loop evaluation?\n",
    "In closed-loop evaluation the model is in **full control of the SDV**. At each time step, we predict the future trajectory and then move the AV to the first of the model's predictions. \n",
    "\n",
    "We refer to this process with the terms **forward-simulate** or **unroll**.\n",
    "\n",
    "![closed-loop](../../images/planning/closed-loop.svg)\n",
    "\n",
    "\n",
    "## What can we use closed-loop evaluation for?\n",
    "Closed-loop is crucial to asses a model's capabilities before deploying it in the real world. **Ideally, we would test the model on the road in the real world**. However, this is clearly very expensive and scales poorly. Forward-simulation is an attempt to evaluate the system in a setting which is as close as possible to a real road test on the same route.\n",
    "\n",
    "Differently from open-loop, the model is now in full control of the SDV and predictions are used to compute the future location of the SDV.\n",
    "\n",
    "## Is closed-loop evaluation enough?\n",
    "Closed-loop evaluation is an important step forward towards evaluating how our policy would perform on the road. However, it still has some limitations.\n",
    "\n",
    "The critical one is the **non-reactivity of other traffic participants**. In fact, while the SDV is now controlled by our policy, other actors are still being replayed from the original log. In this setting, a chasing car will not slow down if our policy choses a different speed profile for the SDV, resulting in a rear collision that wouldn't obviously occur in the real world.\n",
    "\n",
    "For this reason, closed-loop evaluation is only accurate for the first few seconds of forward simulation. This can be mitigated when enough data exists for the task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tempfile import gettempdir\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataloader import default_collate\n",
    "from tqdm import tqdm\n",
    "\n",
    "from l5kit.configs import load_config_data\n",
    "from l5kit.data import LocalDataManager, ChunkedDataset, filter_agents_by_frames\n",
    "from l5kit.dataset import EgoDataset\n",
    "from l5kit.rasterization import build_rasterizer\n",
    "from l5kit.geometry import transform_points, angular_distance, yaw_as_rotation33\n",
    "from l5kit.visualization import TARGET_POINTS_COLOR, PREDICTED_POINTS_COLOR, draw_trajectory\n",
    "from l5kit.planning.utils import detect_collision\n",
    "from l5kit.kinematic import AckermanPerturbation\n",
    "from l5kit.random import GaussianRandomGenerator\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data path and load cfg\n",
    "\n",
    "By setting the `L5KIT_DATA_FOLDER` variable, we can point the script to the folder where the data lies.\n",
    "\n",
    "Then, we load our config file with relative paths and other configurations (rasteriser, training params...)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set env variable for data\n",
    "os.environ[\"L5KIT_DATA_FOLDER\"] = \"/tmp/l5kit_data\"\n",
    "dm = LocalDataManager(None)\n",
    "# get config\n",
    "cfg = load_config_data(\"./config.yaml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"/tmp/planning_model.pt\"\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = torch.load(model_path).to(device)\n",
    "model = model.eval()\n",
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the evaluation data\n",
    "Differently from training and open loop evaluation, this setting is intrinsically sequential. As such, we won't be using any of PyTorch's parallelisation functionalities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== INIT DATASET\n",
    "eval_cfg = cfg[\"val_data_loader\"]\n",
    "rasterizer = build_rasterizer(cfg, dm)\n",
    "eval_zarr = ChunkedDataset(dm.require(eval_cfg[\"key\"])).open()\n",
    "eval_dataset = EgoDataset(cfg, eval_zarr, rasterizer)\n",
    "print(eval_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define our unroll function and metrics\n",
    "\n",
    "Our unroll function is actually very simple. At each timestep:\n",
    "- we **forward the current frame** to our policy;\n",
    "- we **get the predicted trajectory** from the model;\n",
    "- we **compute our metrics**;\n",
    "- if we have not drifted too much from the original data we procede to the next frame;\n",
    "- otherwise, we reset the AV to the original GT position.\n",
    "\n",
    "The function returns not only the RGB frames, but also the different types of errors according to multiple metrics.\n",
    "\n",
    "## Closed-loop metrics\n",
    "In this setting **metrics are particularly challenging**. In fact, we would like to penalise some of the simulation drift (e.g. going off road or in the opposite lane) while at the same time allow others (e.g. different speed profiles).\n",
    "\n",
    "### Collisions\n",
    "Our SDV should avoid collisions with other agents. In this example, we won't distinguish between collisions caused by non-reactivity of other agents and actual collisions, and we will simply report them all categorised by where they occurred (front, rear and side with respect to the AV).\n",
    "\n",
    "However, if we only considered collisions, our SDV could pass all tests by simply driving off-road or in a different lane.\n",
    "\n",
    "### Distance from reference trajectory\n",
    "To address the issue presented above, we require our SDV to loosely stick to the original trajectory in the data. By setting the right threshold on the distance we can allow for different speed profiles and small steerings, while pensalising large deviations like driving off-road.\n",
    "\n",
    "We can do so by computing the distance between the first predictions from the policy and the corresponding annotated positions in world coordinates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unroll_scene(scene_dataset, model, drifting_threshold=10):\n",
    "    ## prepare return buffers\n",
    "    images = []\n",
    "    collisions = []\n",
    "    driftings = []\n",
    "\n",
    "    for frame_idx in tqdm(range(len(scene_dataset))):\n",
    "        data = scene_dataset[frame_idx]\n",
    "        data_batch = default_collate([data])\n",
    "        data_batch = {k: v.to(device) for k, v in data_batch.items()}\n",
    "\n",
    "        result = model(data_batch)\n",
    "        \n",
    "        predicted_positions = result[\"positions\"].detach().cpu().numpy().squeeze()\n",
    "        predicted_yaws = result[\"yaws\"].detach().cpu().numpy().squeeze()\n",
    "\n",
    "        ## store image\n",
    "        im_ego = rasterizer.to_rgb(data[\"image\"].transpose(1, 2, 0))    \n",
    "        draw_trajectory(im_ego, transform_points(data[\"target_positions\"], data[\"raster_from_agent\"]), TARGET_POINTS_COLOR)\n",
    "        draw_trajectory(im_ego, transform_points(predicted_positions, data[\"raster_from_agent\"]), PREDICTED_POINTS_COLOR)\n",
    "        images.append(im_ego)\n",
    "\n",
    "        ## compute absolute positions\n",
    "        pred_positions_m = transform_points(predicted_positions, data[\"world_from_agent\"])\n",
    "        pred_angles_rad = predicted_yaws + data[\"yaw\"]\n",
    "        gt_positions_m = transform_points(data[\"target_positions\"], data[\"world_from_agent\"])\n",
    "\n",
    "        ## detect collisions\n",
    "        agents_frame = filter_agents_by_frames(scene_dataset.dataset.frames[frame_idx], scene_dataset.dataset.agents)[0]\n",
    "        collision = detect_collision(data[\"centroid\"], data[\"yaw\"], data[\"extent\"],agents_frame)\n",
    "        collisions.append(collision)\n",
    "        if collision[0] != \"\":\n",
    "            continue\n",
    "\n",
    "        ## detect drifting\n",
    "        drifting = np.linalg.norm(pred_positions_m[0] - gt_positions_m[0]) > drifting_threshold\n",
    "        driftings.append(drifting)\n",
    "        if drifting:\n",
    "            continue\n",
    "\n",
    "        ## mutate the next frame if we're not at the end of the scene\n",
    "        frame_mutate_idx = frame_idx + 1\n",
    "        if frame_mutate_idx < len(scene_dataset):\n",
    "            scene_dataset.dataset.frames[frame_mutate_idx][\"ego_translation\"][:2] = pred_positions_m[0]\n",
    "            scene_dataset.dataset.frames[frame_mutate_idx][\"ego_rotation\"] = yaw_as_rotation33(pred_angles_rad[0])\n",
    "    \n",
    "    return images, collisions, driftings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the unroll function\n",
    "In this cell, we test our unroll function on a set of scenes from the evaluation set. \n",
    "\n",
    "L5Kit comes with a handy function to separate an individual scene from a bigger dataset that we can use to unroll a single scene (or a set) of choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== EVAL LOOP\n",
    "scenes_to_unroll = 10\n",
    "images, collisions, driftings = [], [], []\n",
    "for scene_idx in range(0, len(eval_zarr.scenes), len(eval_zarr.scenes)//scenes_to_unroll):\n",
    "    scene_dataset = eval_dataset.get_scene_dataset(scene_idx)\n",
    "    scene_images, scene_collisions, scene_driftings = unroll_scene(scene_dataset, model)\n",
    "    images.append(scene_images)\n",
    "    collisions.append(scene_collisions)\n",
    "    driftings.append(scene_driftings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantitative evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting errors from the closed-loop\n",
    "\n",
    "We can collect metrics from different scenes and plot them here.\n",
    "\n",
    "For collision, we have split them into *rear, front* and *side* to better capture the nature of different errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collision_names = np.asarray([collision[0] for scene_collisions in collisions for collision in scene_collisions])\n",
    "driftings = np.asarray([drift for scene_driftings in driftings for drift in scene_driftings])\n",
    "values = []\n",
    "names = []\n",
    "\n",
    "for collision_name in [\"front\", \"side\", \"rear\"]:\n",
    "    values.append(np.sum(collision_names == collision_name))\n",
    "    names.append(collision_name)\n",
    "\n",
    "values.append(np.sum(driftings == True))\n",
    "names.append(\"displacement error\")\n",
    "\n",
    "plt.bar(np.arange(len(names)), values)\n",
    "plt.xticks(np.arange(len(names)), names)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Qualitative evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualise the closed-loop\n",
    "\n",
    "We can visualise one of the example we have stored in the previous cell. \n",
    "\n",
    "**The policy is now in full control of the SDV as this moves through the annotated scene.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, clear_output\n",
    "import PIL\n",
    "from time import sleep\n",
    "\n",
    "for frame in images[0]:\n",
    "    clear_output(wait=True)\n",
    "    display(PIL.Image.fromarray(frame))\n",
    "    sleep(0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-trained model results\n",
    "\n",
    "We include here the unroll of one scene using one of our pre-trained model. The controlled SDV can stick to the correct lane and stops successfully for a red light. \n",
    "\n",
    "Comparing this result with the one from the [open-loop notebook](./open_loop_test.ipynb) we can notice some differences in the speed profile chosen by the model.\n",
    "\n",
    "\n",
    "![SegmentLocal](../../images/planning/out_9_closed.gif \"segment\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
